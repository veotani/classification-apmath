{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Разработка классификатора новостей\n",
    "\n",
    "**Нужно:**\n",
    "* выбрать какой-либо новостной ресурс, где к новостям привязаны категории или метки (например http://lenta.ru, http://fontanka.ru, http://gazeta.ru)\n",
    "* загрузить новости по некоторому набору (5-10) категорий за пару лет\n",
    "* обучить классификатор на эти новостях\n",
    "* продемонстрировать его работу, разработав простеший web-интерфейс (вариант - telegram-бот), куда пользователь вводит текст новости и на выходе получает наиболее вероятную категорию. В качестве фреймворка проще всего взять [Flask](http://flask.pocoo.org) (см. примеры) ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Экземпляр классификатора в пикле, получение в последней ячейке\n",
    "\n",
    "import lxml.html\n",
    "import lxml.etree\n",
    "import requests\n",
    "import pymorphy2\n",
    "import re\n",
    "import math\n",
    "import operator\n",
    "import numpy as np\n",
    "import pickle\n",
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# По календарю получаю список URL ведущих на новости этого дня\n",
    "\n",
    "def parse_category_by_url(url):\n",
    "    response = requests.get(url)\n",
    "    if (response.status_code == 200):\n",
    "        res = []\n",
    "        tree = lxml.html.fromstring(response.text)\n",
    "        links = tree.xpath('//div[contains(@class, \"big_center ml30 fll\")]//td[contains(@class, \"dom\")]')\n",
    "        for link in links:\n",
    "            try:\n",
    "                formed_link = form_url_by_suburl(link.getchildren()[0].get(\"href\"))                \n",
    "                res.append(formed_link)\n",
    "            except:\n",
    "                pass\n",
    "        return res\n",
    "        \n",
    "    return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# По странице с новостями дня получаю список URL ведущих на новость\n",
    "\n",
    "def parse_news_on_page(url):\n",
    "    response = requests.get(url)\n",
    "    if (response.status_code == 200):\n",
    "        res = []\n",
    "        tree = lxml.html.fromstring(response.text)\n",
    "        links = tree.xpath('//div[contains(@class, \"calendar-list\")]//div[contains(@class, \"calendar-item-title\")]')\n",
    "        for link in links:\n",
    "            try:\n",
    "                formed_link = form_url_by_suburl(link.getchildren()[0].get(\"href\"))\n",
    "                if len(formed_link)>0:\n",
    "                    res.append(formed_link)\n",
    "            except:\n",
    "                pass\n",
    "        return res\n",
    "    return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Из относительных ссылок формирую абсолютную\n",
    "\n",
    "def form_url_by_suburl(suburl):\n",
    "    if suburl[0] == '/':\n",
    "        return 'http://www.fontanka.ru' + suburl\n",
    "    return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Получаю текст и заголовок новости\n",
    "\n",
    "def get_content(url):\n",
    "    response = requests.get(url)\n",
    "    if (response.status_code == 200):        \n",
    "        analyzer = pymorphy2.MorphAnalyzer()\n",
    "        #words_from_query = {analyzer.normal_forms(x)[0] for x in query.split(' ')}\n",
    "        tree = lxml.html.fromstring(response.text)\n",
    "        text = tree.xpath('//div[contains(@class, \"article_fulltext\")]')\n",
    "        arttitle = tree.xpath('//h1[contains(@class, \"article_title\")]')\n",
    "        try:\n",
    "            title = arttitle[0].text_content()\n",
    "            #print(text[0].text_content())\n",
    "            res = ''.join(text[0].text_content().replace('\\t', ' ').replace('\\n', ' ').splitlines())\n",
    "        except:\n",
    "            title = ''\n",
    "            res = ['']\n",
    "    return (title, res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Обработка и сохранение новости в файл\n",
    "\n",
    "def save_cat_in_file(url, filename):\n",
    "    day_links = parse_category_by_url(url)\n",
    "    f = open(filename, 'w')\n",
    "    for day_link in tqdm(day_links):\n",
    "        news_links = parse_news_on_page(day_link)\n",
    "        for news_link in news_links:\n",
    "            (title, text) = get_content(news_link)\n",
    "            try:\n",
    "                f.write(f\"{title}\\t{text}\\n\")\n",
    "            except:\n",
    "                pass\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a93cc2f512b6445f8108f92b069007e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d5eeb8ff49e433f952402d28da6a2c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-16:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Veotani\\Anaconda3\\lib\\threading.py\", line 916, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"C:\\Users\\Veotani\\Anaconda3\\lib\\site-packages\\tqdm\\_tqdm.py\", line 144, in run\n",
      "    for instance in self.tqdm_cls._instances:\n",
      "  File \"C:\\Users\\Veotani\\Anaconda3\\lib\\_weakrefset.py\", line 60, in __iter__\n",
      "    for itemref in self.data:\n",
      "RuntimeError: Set changed size during iteration\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c8f2427064f4e7dbcbd3c14af20b2e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4aef6dabdfc2419a87ae6ac2fbbaafb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "189723ef1976406db747400e6d9431f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f98b5d36f854da7a59c8fac7c095797",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "531d1fe4492a4e9eb1b3702083bdd350",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa427879978b4884bc6a44c2bfcd6070",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Нормализуем все слова в файлах\n",
    "\n",
    "def normilize_files():\n",
    "    files = [\n",
    "            'buisness',\n",
    "            'politic',\n",
    "            'realty',\n",
    "            'sport',\n",
    "            'technology',\n",
    "            'tourism',\n",
    "            'video'\n",
    "        ]\n",
    "    analyzer = pymorphy2.MorphAnalyzer()\n",
    "    for file in tqdm_notebook(files):\n",
    "        with open('categories/' + file + '.txt', 'r') as f:\n",
    "            file_list = []\n",
    "            for line in tqdm_notebook(f.readlines()):\n",
    "                line_list = [analyzer.normal_forms(x)[0] for x in line.split(' ')]                \n",
    "                file_list.append(' '.join(line_list))\n",
    "        with open('categories/' + file + '_normal.txt', 'w') as f:\n",
    "            f.writelines(file_list)\n",
    "            \n",
    "normilize_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe992bd610a34d14a407098fcf57e4fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#убираем знаки пунктуации\n",
    "\n",
    "def delete_all_punct():\n",
    "    files = [\n",
    "            'buisness',\n",
    "            'politic',\n",
    "            'realty',\n",
    "            'sport',\n",
    "            'technology',\n",
    "            'tourism',\n",
    "            'video'\n",
    "        ]\n",
    "    analyzer = pymorphy2.MorphAnalyzer()\n",
    "    for file in tqdm_notebook(files):\n",
    "        with open('categories/' + file + '_normal.txt', 'r') as f:\n",
    "            lines = []\n",
    "            for line in f.readlines():\n",
    "                lines.append(re.sub(pattern='[^a-zA-Z]', string=line.replace(',', ' ').replace('.', ' ').replace('-', ' '), repl=' '))\n",
    "        with open('categories/' + file + '_normal_nopun.txt', 'w') as f:\n",
    "            f.writelines(lines)\n",
    "delete_all_punct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_rid_of_stop_words(dict_words):\n",
    "    with open('stopwords.txt', 'r') as f:\n",
    "        for line in f.readlines():\n",
    "            if line in dict_words:\n",
    "                del dict_words[line]\n",
    "    return dict_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all the words in our collection\n",
    "\n",
    "def get_all_words(cat='notcat'):\n",
    "    res = dict()\n",
    "    files = [\n",
    "            'buisness',\n",
    "            'politic',\n",
    "            'realty',\n",
    "            'sport',\n",
    "            'technology',\n",
    "            'tourism',\n",
    "            'video'\n",
    "        ]\n",
    "    if (cat in files):\n",
    "        with open('categories/' + cat + '_normal_nopun.txt', 'r') as f:\n",
    "            for line in f.readlines():\n",
    "                for word in line.split(' '):\n",
    "                    if word not in res:\n",
    "                        res[word] = 1\n",
    "                    else:\n",
    "                        res[word] = res[word] + 1\n",
    "    else:\n",
    "        for file in files:\n",
    "            with open('categories/' + file + '_normal_nopun.txt', 'r') as f:\n",
    "                for line in f.readlines():\n",
    "                    for word in line.split(' '):\n",
    "                        if word not in res:\n",
    "                            res[word] = 1\n",
    "                        else:\n",
    "                            res[word] = res[word] + 1\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_docs_in_cat(category):\n",
    "    f = open('categories/' + category + '_normal.txt', 'r')\n",
    "    return len(f.readlines())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_all_docs():\n",
    "    files = [\n",
    "            'buisness',\n",
    "            'politic',\n",
    "            'realty',\n",
    "            'sport',\n",
    "            'technology',\n",
    "            'tourism',\n",
    "            'video'\n",
    "        ]\n",
    "    res = 0\n",
    "    for file in files:\n",
    "        res = res + count_docs_in_cat(file)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_uniq_words():\n",
    "    return len(get_all_words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words_in_cat(cat):\n",
    "    res = 0\n",
    "    with open('categories/' + cat + '_normal.txt', 'r') as f:\n",
    "        for line in f.readlines():\n",
    "             res = res + len(line.split(' '))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_parameters():\n",
    "    d = dict()\n",
    "    categories = [\n",
    "            'buisness',\n",
    "            'politic',\n",
    "            'realty',\n",
    "            'sport',\n",
    "            'technology',\n",
    "            'tourism',\n",
    "            'video'\n",
    "        ]\n",
    "    for cat in categories:\n",
    "        d[cat] = (count_docs_in_cat(cat), count_words_in_cat(cat))\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Clasterizator:\n",
    "    def __init__(self):\n",
    "        self.count_docs = count_all_docs()\n",
    "        self.uniq = count_uniq_words()\n",
    "        self.params = get_parameters()\n",
    "        self.categories = [\n",
    "            'buisness',\n",
    "            'politic',\n",
    "            'realty',\n",
    "            'sport',\n",
    "            'technology',\n",
    "            'tourism',\n",
    "            'video'\n",
    "        ]\n",
    "        self.tf_cat = self.init_tf_cat()\n",
    "    def get_cat_of_doc(self, doc):\n",
    "        d = dict()\n",
    "        analyzer = pymorphy2.MorphAnalyzer()\n",
    "        for cat in self.categories:\n",
    "            words_normilized = [analyzer.normal_forms(x) for x in doc.split(' ')]\n",
    "            tmp = 0\n",
    "            for word in words_normilized:\n",
    "                try:\n",
    "                    tf_in_cat = self.tf_cat[cat][word]\n",
    "                except:\n",
    "                    tf_in_cat = 0\n",
    "                tmp = tmp + math.log((tf_in_cat + 1)/(self.uniq + self.params[cat][1]))\n",
    "            d[cat] = math.log(self.params[cat][0]/self.count_docs) + tmp\n",
    "        print(d)\n",
    "        return max(d, key=d.get)\n",
    "\n",
    "        \n",
    "    def init_tf_cat(self):\n",
    "        res = dict()\n",
    "        for cat in self.categories:\n",
    "            res[cat] = get_all_words(cat)\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "cstr = Clasterizator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'buisness': -15.054910981351558, 'politic': -14.97266037465744, 'realty': -15.134650547845972, 'sport': -14.952076946263457, 'technology': -15.16423472806951, 'tourism': -15.136207182692202, 'video': -14.750921110238195}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'video'"
      ]
     },
     "execution_count": 311,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cstr.get_cat_of_doc('судьба')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('cstr.pickle', 'wb') as f:\n",
    "    pickle.dump(cstr, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-324-ae2117947fd9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m#token = токен\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mbot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtelebot\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTeleBot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mcstr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mClasterizator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m@\u001b[0m\u001b[0mbot\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage_handler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcontent_types\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"text\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-304-5c424d2520cf>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     13\u001b[0m             \u001b[1;34m'video'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         ]\n\u001b[1;32m---> 15\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtf_cat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minit_tf_cat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_cat_of_doc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[0md\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-304-5c424d2520cf>\u001b[0m in \u001b[0;36minit_tf_cat\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     34\u001b[0m         \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mcat\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcategories\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m             \u001b[0mres\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcat\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_all_words\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-297-a9fc3f8b2772>\u001b[0m in \u001b[0;36mget_all_words\u001b[1;34m(cat)\u001b[0m\n\u001b[0;32m     17\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m' '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m                         \u001b[0mres\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m                     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m                         \u001b[0mres\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import telebot\n",
    "\n",
    "with open('cstr.pickle', 'rb') as f:\n",
    "     cstr = pickle.load(f)\n",
    "#token = токен\n",
    "bot = telebot.TeleBot(token)\n",
    "cstr = Clasterizator()\n",
    "\n",
    "@bot.message_handler(content_types=[\"text\"])\n",
    "def repeat_all_messages(message): # Название функции не играет никакой роли, в принципе\n",
    "    bot.send_message(message.chat.id, cstr.get_cat_of_doc(message.text))\n",
    "    print(message.text)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "     bot.polling(none_stop=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
