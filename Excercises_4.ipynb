{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Разработка классификатора новостей\n",
    "\n",
    "**Нужно:**\n",
    "* выбрать какой-либо новостной ресурс, где к новостям привязаны категории или метки (например http://lenta.ru, http://fontanka.ru, http://gazeta.ru)\n",
    "* загрузить новости по некоторому набору (5-10) категорий за пару лет\n",
    "* обучить классификатор на эти новостях\n",
    "* продемонстрировать его работу, разработав простеший web-интерфейс (вариант - telegram-бот), куда пользователь вводит текст новости и на выходе получает наиболее вероятную категорию. В качестве фреймворка проще всего взять [Flask](http://flask.pocoo.org) (см. примеры) ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Veotani\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import lxml.html\n",
    "import lxml.etree\n",
    "import requests\n",
    "\n",
    "import pymorphy2\n",
    "import re\n",
    "import math\n",
    "import operator\n",
    "import numpy as np\n",
    "import pickle\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.cross_validation import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Парсинг\n",
    "** Выбран ресурс fontanka. Берутся новости за 2 года (не больше 2000 новостей на одну тему). **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# По календарю получаю список URL ведущих на новости этого дня\n",
    "\n",
    "def parse_category_by_url(url):\n",
    "    response = requests.get(url)\n",
    "    if (response.status_code == 200):\n",
    "        res = []\n",
    "        tree = lxml.html.fromstring(response.text)\n",
    "        links = tree.xpath('//div[contains(@class, \"big_center ml30 fll\")]//td[contains(@class, \"dom\")]')\n",
    "        for link in links:\n",
    "            try:\n",
    "                formed_link = form_url_by_suburl(link.getchildren()[0].get(\"href\"))                \n",
    "                res.append(formed_link)\n",
    "            except:\n",
    "                pass\n",
    "        return res\n",
    "        \n",
    "    return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# По странице с новостями дня получаю список URL ведущих на новость\n",
    "\n",
    "def parse_news_on_page(url):\n",
    "    response = requests.get(url)\n",
    "    if (response.status_code == 200):\n",
    "        res = []\n",
    "        tree = lxml.html.fromstring(response.text)\n",
    "        links = tree.xpath('//div[contains(@class, \"calendar-list\")]//div[contains(@class, \"calendar-item-title\")]')\n",
    "        for link in links:\n",
    "            try:\n",
    "                formed_link = form_url_by_suburl(link.getchildren()[0].get(\"href\"))\n",
    "                if len(formed_link)>0:\n",
    "                    res.append(formed_link)\n",
    "            except:\n",
    "                pass\n",
    "        return res\n",
    "    return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Из относительных ссылок формирую абсолютную\n",
    "\n",
    "def form_url_by_suburl(suburl):\n",
    "    if suburl[0] == '/':\n",
    "        return 'http://www.fontanka.ru' + suburl\n",
    "    return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Получаю текст и заголовок новости\n",
    "\n",
    "def get_content(url):\n",
    "    response = requests.get(url)\n",
    "    if (response.status_code == 200):        \n",
    "        analyzer = pymorphy2.MorphAnalyzer()\n",
    "        #words_from_query = {analyzer.normal_forms(x)[0] for x in query.split(' ')}\n",
    "        tree = lxml.html.fromstring(response.text)\n",
    "        text = tree.xpath('//div[contains(@class, \"article_fulltext\")]')\n",
    "        arttitle = tree.xpath('//h1[contains(@class, \"article_title\")]')\n",
    "        try:\n",
    "            title = arttitle[0].text_content()\n",
    "            #print(text[0].text_content())\n",
    "            res = ''.join(text[0].text_content().replace('\\t', ' ').replace('\\n', ' ').splitlines())\n",
    "        except:\n",
    "            title = ''\n",
    "            res = ['']\n",
    "    try:\n",
    "        return (title, res)\n",
    "    except:\n",
    "        return ('', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Обработка и сохранение новости в файл\n",
    "\n",
    "def save_cat_in_file(url, filename):\n",
    "    day_links = parse_category_by_url(url)\n",
    "    f = open(filename, 'w')\n",
    "    for day_link in tqdm(day_links):\n",
    "        news_links = parse_news_on_page(day_link)\n",
    "        for news_link in news_links:\n",
    "            (title, text) = get_content(news_link)\n",
    "            try:\n",
    "                f.write(f\"{title}\\t{text}\\n\")\n",
    "            except:\n",
    "                pass\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2c682093d3848abba1a0caf47f2867d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def get_big_data():\n",
    "    date_cat = re.compile(r'/(\\w*)/')\n",
    "    links = []\n",
    "    start_url = \"http://www.fontanka.ru/fontanka/arc/2015/all.html\"\n",
    "    response = requests.get(start_url)\n",
    "    if (response.status_code == 200):\n",
    "        tree_categories = lxml.html.fromstring(response.text)\n",
    "        categories_links = tree_categories.xpath('//div[contains(@class, \"calendar-menu\")]//select[contains(@name, \"categoryselect\")]//option')\n",
    "        if (len(categories_links) > 1):\n",
    "            for category_link in tqdm_notebook(categories_links):\n",
    "                    documents_count = 0\n",
    "                    url = form_url_by_suburl(category_link.get(\"value\"))\n",
    "                    \n",
    "                    \n",
    "                   ################################################################################################################################### \n",
    "                   # Файлы, которые уже обработал, но вылетела ошибка. Если надо снова пропарсить, закомментить!!                                    #\n",
    "                   # already_processed = ['money', 'spb', 'media', 'politic', 'fontantv', 'bt', 'business', 'culturecity', 'autop', 'charity', 'zhkh']#\n",
    "                   ###################################################################################################################################\n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                    if (date_cat.findall(url)[1] == 'fontanka' or date_cat.findall(url)[1] in already_processed):\n",
    "                        continue\n",
    "                    with open(date_cat.findall(url)[1] + '.txt', 'w') as f:\n",
    "                        response_category = requests.get(url)\n",
    "                        tree_year = lxml.html.fromstring(response_category.text)\n",
    "                        years = tree_year.xpath('//div[contains(@class, \"calendar-menu\")]//select[contains(@name, \"yearselect\")]//option')\n",
    "                        if (len(years) > 1):\n",
    "                            years_count = 0\n",
    "                            for year in years:                    \n",
    "                                days_links = parse_category_by_url(form_url_by_suburl(year.get(\"value\")))\n",
    "                                for day_link in days_links:\n",
    "                                    news_link = parse_news_on_page(day_link)\n",
    "                                    for new_link in news_link:\n",
    "                                        if documents_count > 2500:\n",
    "                                            continue # Раз по кайфу, питон, без проблем -- продолжай\n",
    "                                        (title, text) = get_content(new_link)\n",
    "                                        try:\n",
    "                                            f.write((title + ' ' + text).replace('\\n', ' ') + '\\n')\n",
    "                                            documents_count = documents_count + 1\n",
    "                                   \n",
    "                                        except:\n",
    "                                            pass\n",
    "                                years_count = years_count + 1\n",
    "                                if years_count == 2:\n",
    "                                    break\n",
    "get_big_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['autop.txt', 'business.txt', 'cat_doc.txt', 'finances.txt', 'fontantv.txt', 'incd.txt', 'media.txt', 'politic.txt', 'realty.txt', 'society.txt', 'spb.txt', 'sport.txt', 'stroy.txt', 'technology.txt', 'turizm.txt', 'witness.txt', 'zhkh.txt']\n"
     ]
    }
   ],
   "source": [
    "# Будем работать только с файлами где больше 500 документов\n",
    "\n",
    "valid_files = []\n",
    "import os\n",
    "for file in os.listdir():\n",
    "    if \".txt\" in file:\n",
    "        with open(file, 'r') as f:\n",
    "            count_lines = 0\n",
    "            for line in f.readlines():\n",
    "                count_lines = count_lines + 1\n",
    "            if count_lines > 500:\n",
    "                valid_files.append(file)\n",
    "print(valid_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Убираем стоп слова из словаря\n",
    "\n",
    "def get_rid_of_stop_words(dict_words):\n",
    "    with open('stopwords.txt', 'r') as f:\n",
    "        for line in f.readlines():\n",
    "            for word in line.split(' '):\n",
    "                if word in dict_words:\n",
    "                    del dict_words[word]\n",
    "    return dict_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Выполнение обработки распарсенных файлов с новостями: все слова приводятся к начальной форме, убираются знаки и т.п.\n",
    "\"\"\"\n",
    "\n",
    "def normilize_file(infilename, outfilename):\n",
    "    analyzer = pymorphy2.MorphAnalyzer()\n",
    "    with open(infilename, 'r') as infile, open(outfilename, 'w') as outfile:\n",
    "        for line in tqdm_notebook(infile.readlines()):\n",
    "            words = (word for word in re.split('\\W+', line) if len(word) > 0)\n",
    "            norm_form = (analyzer.normal_forms(word)[0] for word in words)\n",
    "            outfile.write(' '.join(norm_form) + '\\n')\n",
    "\n",
    "def normilize_all_files():\n",
    "    for file in valid_files:\n",
    "        infile = file\n",
    "        outfile = 'normilized_categories/' + file\n",
    "        normilize_file(infile, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Обрабатываем тексты\n",
    "\n",
    "import os\n",
    "\n",
    "not_normilized_fs = os.listdir('no_normilize/')\n",
    "analyzer = pymorphy2.MorphAnalyzer()\n",
    "for file in not_normilized_fs:\n",
    "    with open('no_normilize/' + file, 'r') as f, open('normilize/' + file, 'w') as o:\n",
    "          for line in f.readlines():\n",
    "            words = (word for word in re.split('\\W+', line) if len(word) > 0)\n",
    "            norm_form = (analyzer.normal_forms(word)[0] for word in words)\n",
    "            o.write(' '.join(norm_form) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# разделяем выборки: 80% отсавляем под обучение, 20% под тесты\n",
    "\n",
    "import os\n",
    "\n",
    "not_normilized_fs = os.listdir('normilize/')\n",
    "analyzer = pymorphy2.MorphAnalyzer()\n",
    "for file in not_normilized_fs:\n",
    "    with open('normilize/' + file, 'r') as f:\n",
    "        count_documents = sum(1 for _ in f)\n",
    "    with open('normilize/' + file) as f: \n",
    "        with open('train_data.txt', 'w') as td, open('test_data.txt', 'w') as test:\n",
    "            count_lines = 0\n",
    "            for line in f.readlines():\n",
    "                \n",
    "                if count_lines < count_documents * 0.8:\n",
    "\n",
    "                    td.write(file.reaplce('.txt', '') + '\\t' + line)\n",
    "                    count_lines = count_lines + 1\n",
    "                else:\n",
    "                    test.write(file.reaplce('.txt', '') + '\\t' + line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Обучение\n",
    "** Используются решения Scikit Learn. Выбранный классификатор - SVM (хочется познакомиться + подходящая область применения). **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1621 business\n",
      "4122 cat_doc\n",
      "8964 finances\n",
      "11463 fontantv\n",
      "12380 incd\n",
      "14881 media\n",
      "17382 politic\n",
      "19883 realty\n",
      "21151 society\n",
      "23652 spb\n",
      "26153 sport\n",
      "28654 stroy\n",
      "30259 technology\n",
      "31246 turizm\n",
      "32170 witness\n",
      "33839 zhkh\n"
     ]
    }
   ],
   "source": [
    "# Обучение\n",
    "\n",
    "def get_learned_svm():\n",
    "    docs = []\n",
    "    y = []\n",
    "    with open('train_data.txt', 'r') as f:\n",
    "        for line in f.readlines():\n",
    "            docs.append(line.split('\\t')[1])\n",
    "            y.append(line.split('\\t')[0])\n",
    "    vectorizer = TfidfVectorizer(min_df=1)\n",
    "    X = vectorizer.fit_transform(docs)\n",
    "    svm = SVC(kernel='linear', verbose=True)\n",
    "    svm.fit(X, y)\n",
    "    \n",
    "    docs_train = []\n",
    "    y_train = []\n",
    "    with open('test_data.txt', 'r') as f:\n",
    "        for line in f.readlines():\n",
    "            docs_test.append(line.split('\\t')[1])\n",
    "            y_test.append(line.split('\\t')[0])\n",
    "    \n",
    "    vectorizer_test = TfidfVectorizer(min_df=1, vocabulary=vectorizer.vocabulary_)\n",
    "    X_test = vectorizer.fit_transform(docs_test)\n",
    "    pred = svm.predict(X_test)\n",
    "    print('На тестовых данных: ')\n",
    "    print(svm.score(X_test, y_test))\n",
    "    return svm, vectorizer.vocabulary_\n",
    "\n",
    "get_learned_svm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialize_tfidf():\n",
    "    docs = []\n",
    "    y = []\n",
    "    with open('cat_doc.txt', 'r') as f:\n",
    "        docs_in_category = 0\n",
    "        for line in f.readlines():\n",
    "            docs.append(line.split('\\t')[1])\n",
    "            y.append(line.split('\\t')[0])\n",
    "    vectorizer = TfidfVectorizer(min_df=1)\n",
    "    X = vectorizer.fit_transform(docs)\n",
    "    return vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'politic'"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect = vectorizer_to_our.fit_transform(['popka'])\n",
    "cat = svm.predict(vect)\n",
    "cat[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
